{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "kaxOy2isb5xs", "tags": []}, "outputs": [], "source": "%matplotlib inline"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "bKOoRFl3bMwG", "tags": []}, "source": "# Basics of PyTorch\n\nWe start by running some basic PyTorch commands to get acquainted with the library. This section is based on the [PyTorch example](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) in the official tutorials.\n\nNumpy is a generic Python library for scientific computing and allows to work with $n$-dimensional arrays."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "id": "5Qv1SDicbMwI", "outputId": "c8544ae8-85de-4c77-c5b0-8874465f2080", "tags": []}, "outputs": [], "source": "# load numpy\nimport numpy as np\n\n# create standard normally distributed vectors x and y\nx = np.random.randn(20)\ny = np.random.randn(20)\n\n# compute the standard inner product of x and y\nnp.dot(x, y)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "2Z5bRRG6bMwP", "tags": []}, "source": "In principle, one can implement even neural network models using numpy. However, numpy has two severe limitations. When working with deep neural networks, often one wants to speed up the computations by running them on GPUs. Unfortunately, numpy does not support GPUs. Moreover, training of differentiable models such as neural networks requires to keep track of gradients. This functionality is not provided by numpy.\n\nThe PyTorch library tries to solve both issues."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# load PyTorch\nimport torch"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "You can check that you use a recent version of PyTorch (as of September 2021, the most recent version is 1.9.0):"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "torch.__version__"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "2Z5bRRG6bMwP", "tags": []}, "source": "Similar to numpy, PyTorch allows us to work with $n$-dimensional arrays, which are called *tensors*."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "id": "dTsIolBDbMwQ", "outputId": "c9e7e877-e3a5-47d1-df10-6e31f651c23b", "tags": []}, "outputs": [], "source": "# create standard normally distributed vectors x and y\nx = torch.randn(20)\ny = torch.randn(20)\n\n# compute the standard inner product of x and y\ntorch.dot(x, y)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "N8zXqQ-lbMwV", "tags": []}, "source": "We can easily convert numpy arrays to PyTorch tensors:"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 69}, "colab_type": "code", "id": "kc952VBIbMwX", "outputId": "85a8367d-b25b-4c0e-a6b4-04589104410f", "tags": []}, "outputs": [], "source": "a = np.random.randn(20)\ntorch.from_numpy(a)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "GqQ9Fe7ct1sr", "tags": []}, "source": "By default numpy uses 64 bit floating point numbers, which are preserved by the conversion to PyTorch tensors. However, floating point numbers in PyTorch are encoded with 32 bits by default. The standard floating point representation in PyTorch can be obtained by running"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 69}, "colab_type": "code", "id": "VzZ-SjfvuHxn", "outputId": "9e8b2570-7499-4535-9aa2-09acdccae2b3", "tags": []}, "outputs": [], "source": "torch.from_numpy(a).float()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Ol8qvEV5bMwb", "tags": []}, "source": "Similarly, PyTorch tensors can be converted to numpy arrays. Note that in this case an array with 32 bit floating point numbers is returned:"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 104}, "colab_type": "code", "id": "3gS5yeKLbMwd", "outputId": "796085cb-e188-4b87-a0f2-f5e162971367", "tags": []}, "outputs": [], "source": "b = torch.randn(20)\nb.numpy()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "CQDiZMgobMwj", "tags": []}, "source": "An advantage of PyTorch over numpy is that we can utilize GPUs by moving our data and models to the GPU. Note that the following example is only executed if you have access to a GPU."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "MptuL0OdbMwk", "tags": []}, "outputs": [], "source": "if torch.cuda.is_available():\n    x = x.cuda()\n    y = y.cuda()\n    print(torch.dot(x, y))"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "wfYh85bPbMwo", "tags": []}, "source": "In the same way, data and models can be moved back to the CPU."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "4igwvkDIbMwq", "tags": []}, "outputs": [], "source": "x = x.cpu()\ny = y.cpu()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "0vl6ZqrrbMwu", "tags": []}, "source": "If we do not want to check `torch.cuda.is_available()` repeatedly, we can also define a device on which we would like to perform our computations and move tensors to this device."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "9IhmlX4HbMwv", "tags": []}, "outputs": [], "source": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nx = x.to(device)\ny = y.to(device)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "IL2WxVqObMw0", "tags": []}, "source": "As mentioned above, often differentiable models are trained by gradient descent. To perform gradient descent, we have to know the gradient of the objective function with respect to the model parameters. In neural networks, these gradients can be computed efficiently by using the [backpropagation algorithm](https://en.wikipedia.org/wiki/Backpropagation).\n\nDuring the forward pass in our model, PyTorch defines a *computational graph*, whose nodes are PyTorch tensors and whose edges are the functions mapping one input tensor to another output tensor. By backpropagating this graph, we obtain the desired gradients. \n\nMore concretely, if `x` is a tensor with attribute `x.requires_grad = True` then `x.grad` will be another tensor holding the gradient of `x` with respect to some scalar value."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 86}, "colab_type": "code", "id": "NGtyB-0FbMw2", "outputId": "4954259c-b091-4919-c9e4-605219875a42", "tags": []}, "outputs": [], "source": "# state that we want to obtain the gradients of x\nx.requires_grad = True\n\n# run the forward pass and build the computational graph\nz = torch.sum(x - y)\nprint(f\"Output of the forward pass: {z}\")\n\n# backpropagate the graph\nz.backward()\n\nprint(f\"Gradient information of x: {x.grad}\")\nprint(f\"Gradient information of y: {y.grad}\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "pIt_w1AMvHOx", "tags": []}, "source": "By default, if we repeat our computations and backpropagate again, the gradients are accumulated."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 86}, "colab_type": "code", "id": "c3cmFXm9vimy", "outputId": "81ed0cbb-d06d-4a31-aef9-f3ab86de914d", "tags": []}, "outputs": [], "source": "# run the forward pass and build the computational graph\nz = torch.sum(x - y)\nprint(f\"Output of the forward pass: {z}\")\n\n# backpropagate the graph\nz.backward()\n\nprint(f\"Gradient information of x: {x.grad}\")\nprint(f\"Gradient information of y: {y.grad}\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "0-B_Aa4QvofM", "tags": []}, "source": "To override the gradients, we have to set them to zero before running the forward pass."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 86}, "colab_type": "code", "id": "axPworlwvoI_", "outputId": "5059624a-2ff4-4437-ee5e-41b996bc2a27", "tags": []}, "outputs": [], "source": "# set the gradient of x to zero\nx.grad.zero_()\n\n# run the forward pass and build the computational graph\nz = torch.sum(x - y)\nprint(f\"Output of the forward pass: {z}\")\n\n# backpropagate the graph\nz.backward()\n\nprint(f\"Gradient information of x: {x.grad}\")\nprint(f\"Gradient information of y: {y.grad}\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "7TxyaB_NbMw6", "tags": []}, "source": "If we want perform operations that should not be tracked by PyTorch, we can wrap them inside a `torch.no_grad()` clause."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 34}, "colab_type": "code", "id": "KTL67owpbMw7", "outputId": "eccb6417-9bd0-47b8-bed0-d0a8f882ac7e", "scrolled": true, "tags": []}, "outputs": [], "source": "with torch.no_grad():\n    print(torch.sum(x - y))"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "With recent PyTorch versions an error is thrown if we try to convert a PyTorch tensor with gradient information to a numpy array:"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "x.numpy()"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "As suggested by the error message, we first have to remove the gradient information with [`detach`](https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html) (we \"detach\" the tensor from the computational graph):"}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "x.detach().numpy()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "buOJk_X-bMxA", "tags": []}, "source": "# Linear regression example\n\nNow we do something (slightly) more interesting. We consider linear regression using PyTorch. We study the model\n\\begin{equation*}\n    y = \\theta_0 + \\theta_1 x + \\epsilon,\n\\end{equation*}\nwhere $\\epsilon$ represents the noise term.\n\nWe generate training data $\\{x_i, y_i\\}_{i=1}^N$ with $N = 100$ data points from the model with the true parameters $\\boldsymbol{\\theta} = \\begin{bmatrix} \\theta_0 & \\theta_1\\end{bmatrix}^\\intercal = \\begin{bmatrix} 0.3 & 0.1 \\end{bmatrix}^\\intercal$ and normally distributed noise $\\epsilon \\sim \\mathcal{N}(0, 0.01^2)$. The inputs $x_i$ are sampled uniformly from the interval $[0,1]$."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "JD0sMA4zbMxB", "tags": []}, "outputs": [], "source": "theta_true = [0.3, 0.1]\nN = 100\n\nnp.random.seed(1234)\nxt = np.random.uniform(size=N)\nyt = theta_true[0] + theta_true[1] * xt + 0.01 * np.random.randn(N)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "rUUhoKhwbMxG", "tags": []}, "source": "We plot the data to see what it looks like."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 269}, "colab_type": "code", "id": "OLRAfKcObMxH", "outputId": "f8a79848-f58c-4879-ec05-82cc899defa9", "tags": []}, "outputs": [], "source": "import matplotlib\nimport matplotlib.pyplot as plt\n\nplt.scatter(xt, yt, label='data')\nplt.plot([0, 1], [theta_true[0], theta_true[0] + theta_true[1]], 'k--', label='true model')\nplt.legend()\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "fIilymh9p4oK", "tags": []}, "source": "We want to estimate the parameters $\\boldsymbol{\\theta} = \\begin{bmatrix} \\theta_0 & \\theta_1\\end{bmatrix}^\\intercal$ of the linear regression model based on the training data $\\{x_i,y_i\\}_{i=1}^N$. The training data statistics can be written as\n\\begin{equation*}\n  \\mathbf{y} = \\mathbf{X} \\boldsymbol{\\theta} + \\boldsymbol{\\epsilon}, \\qquad \\mathbf{y}= \\begin{bmatrix} y_1 \\\\ \\vdots \\\\ y_N \\end{bmatrix}, \\qquad \\mathbf{X} = \\begin{bmatrix} 1 & x_1 \\\\ \\vdots & \\vdots \\\\ 1 & x_N \\end{bmatrix}, \\qquad \\boldsymbol{\\theta} = \\begin{bmatrix} \\theta_0 \\\\ \\theta_1 \\end{bmatrix},\n\\end{equation*}\nwhere $\\boldsymbol{\\epsilon}$ is a vector with the noise realizations. This formulation also adheres to the common convention in PyTorch that expects data to be multi-dimensional with each row representing a data sample. Hence we even reshape the vector of outputs to a matrix with one column by using `view(-1, 1)`."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "dJvA1fEQs8b_", "tags": []}, "outputs": [], "source": "# build the data matrices\nX = torch.stack([torch.ones(N), torch.from_numpy(xt).float()], dim = 1)\nY = torch.from_numpy(yt).float().view(-1, 1)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "825xo6yKGzf1", "tags": []}, "source": "We also need to create a tensor for $\\boldsymbol{\\theta}$, which will be updated iteratively during the learning process. We initialize it with the constant  matrix $\\boldsymbol{\\theta} = \\begin{bmatrix} 1 & 1 \\end{bmatrix}^\\intercal$ and specify that we want to obtain the gradient of the (not yet defined) loss with respect to $\\boldsymbol{\\theta}$ during backpropagation."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "b4Y0q6iYHLuQ", "tags": []}, "outputs": [], "source": "# define the initial model parameters\ntheta = torch.ones(2, 1, requires_grad = True)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "kIaUrrm6IAh_", "tags": []}, "source": "We proceed by implementing the mean squared error cost function $J(\\boldsymbol{\\theta})$, where\n\\begin{equation*}\nJ(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_{i=1}^N {(\\theta_0 + \\theta_1 x_i \u2212 y_i)}^2 = \\frac{1}{n} {\\|\\mathbf{X} \\boldsymbol{\\theta} \u2212 \\mathbf{y} \\|}^2_2.\n\\end{equation*}\nWe multiply the input $\\mathbf{X}$ with the variable $\\boldsymbol{\\theta}$ using `mm` for matrix multiplication. We then subtract the corresponding outputs, take the square, and finally average over all data samples."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "GL_kqJTtJfhS", "tags": []}, "outputs": [], "source": "# evaluate the mean squared error for the current parameters\nloss = (X.mm(theta) - Y).pow(2).mean()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "4sMWBSBBK-J6", "tags": []}, "source": "We perform the optimization iteratively with gradient descent. This means that in every optimization step we update the parameter $\\boldsymbol{\\theta}$ by going in the opposite direction of the gradient of the cost function with a certain step length $\\gamma$, i.e., we compute $\\boldsymbol{\\theta} := \\boldsymbol{\\theta} - \\gamma \\nabla J(\\boldsymbol{\\theta})$. In this example we choose $\\gamma = 0.1$. In PyTorch the update step can be implemented as"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "QGAsamm1LrF0", "tags": []}, "outputs": [], "source": "# perform backpropagation\nloss.backward()\n\n# perform a gradient descent step\nwith torch.no_grad():\n    theta -= 0.1 * theta.grad"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "LvcYU2ATbMxL", "tags": []}, "source": "Before we continue with the next optimization step, we have to reset the gradient to zero.\n"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 52}, "colab_type": "code", "id": "xJWXRL9HNIOE", "outputId": "490c6534-22c2-4220-d2f5-c0c26b1c3611", "tags": []}, "outputs": [], "source": "# reset the gradient information\ntheta.grad.zero_()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "xGxdwMU9NEq3", "tags": []}, "source": "The training step described above can now be repeated multiple times in a `for` loop. As an additional feature we create an illustration of the loss during the course of the optimization procedure. "}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 474}, "colab_type": "code", "id": "BPOPNi14bMxN", "outputId": "889f2098-4d64-4216-cbec-faf36c8399db", "tags": []}, "outputs": [], "source": "# define the initial model parameters\ntheta = torch.ones(2, 1, requires_grad = True)\n\n# track the training loss\ntraining_loss = []\n\n# in multiple iterations\nfor i in range(1000):\n    # evaluate the mean squared error for the current parameters\n    loss = (X.mm(theta) - Y).pow(2).mean()\n    \n    # update the statistic and print loss\n    training_loss.append(loss.item())\n    if i == 0 or (i + 1) % 100 == 0:\n      print(f\"Epoch {i + 1:4d}: training loss {training_loss[-1]: 9.6f}\")\n    \n    # perform backpropagation\n    loss.backward()\n\n    # perform a gradient descent step\n    with torch.no_grad():\n        theta -= 0.1 * theta.grad\n        \n        # reset the gradient information\n        theta.grad.zero_()\n\nplt.figure()\nplt.plot(np.arange(1, len(training_loss) + 1), training_loss, 'o', label='training loss')\nplt.yscale('log')\nplt.xlabel('number of iterations')\nplt.legend()\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "yMz5Nrg9bMxQ", "tags": []}, "source": "We can check visually that the estimated model is actually close to the true model."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 269}, "colab_type": "code", "id": "S6vJ5dw8bMxR", "outputId": "b43dc913-01ee-4e91-9aaa-a324f4b5c3e6", "tags": []}, "outputs": [], "source": "# we have to detach from the graph since otherwise the automatic conversion\n# to a numpy array performed by matplotlib will error\ntheta_numpy = theta.detach().numpy()\n\nplt.scatter(xt, yt, label = 'data')\nplt.plot([0, 1], [theta_true[0], theta_true[0] + theta_true[1]], 'k--', label = 'true model')\nplt.plot([0, 1], [theta_numpy[0], theta_numpy[0] + theta_numpy[1]], 'r-', label = 'estimated model')\nplt.legend()\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "yGZu48hnbMxV", "tags": []}, "source": "The optimization of model parameters is a very common task in machine learning. PyTorch provides a [library of different optimization algorithms](https://pytorch.org/docs/stable/optim.html) which simplify this procedure, in particular for models with many parameters. The following code snippet defines an `optimizer` that can be used to optimize the parameters `theta` by regular gradient descent with a learning rate of `0.1`."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "tj3VrUZK0M-E", "tags": []}, "outputs": [], "source": "import torch.optim as optim\n\n# define the initial model parameters\ntheta = torch.ones(2, 1, requires_grad = True)\n\n# define the optimizer\noptimizer = optim.SGD([theta], lr=0.1)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "4EmGKUIl09VV", "tags": []}, "source": "After we have computed the gradients for the model parameters that we want to optimize, `optimizer.step()` performs one gradient descent step. The gradients of all optimized parameters can be set to zero by running `optimizer.zero_grad()`.\n\nTaken together, the training of `theta` reduces to the following procedure."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 474}, "colab_type": "code", "id": "y5Eqd5yAbMxW", "outputId": "9d9f7d58-be09-4572-b158-8b886044b6fb", "tags": []}, "outputs": [], "source": "# define the initial model parameters\ntheta = torch.ones(2, 1, requires_grad = True)\n\n# define the optimizer\noptimizer = optim.SGD([theta], lr=0.1)\n\n# track the training loss\ntraining_loss = []\n\n# in multiple iterations\nfor i in range(1000):\n    # evaluate the mean squared error for the current parameters\n    loss = (X.mm(theta) - Y).pow(2).mean()\n    \n    # update the statistic and print loss\n    training_loss.append(loss.item())\n    if i == 0 or (i + 1) % 100 == 0:\n      print(f\"Epoch {i + 1:4d}: training loss {training_loss[-1]: 9.6f}\")\n    \n    # perform backpropagation\n    loss.backward()\n\n    # perform a gradient descent step\n    optimizer.step()\n\n    # reset the gradient information\n    optimizer.zero_grad()\n    \nplt.figure()\nplt.plot(np.arange(1, len(training_loss) + 1), training_loss, 'o', label='training loss')\nplt.yscale('log')\nplt.xlabel('number of iterations')\nplt.legend()\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Q2v09fjUbMxZ", "tags": []}, "source": "Again we inspect our estimated model visually."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 269}, "colab_type": "code", "id": "p8A-te2HbMxb", "outputId": "1b7a1a99-10ea-4998-f2a9-5a22205fca3d", "tags": []}, "outputs": [], "source": "theta_numpy = theta.detach().numpy()\n\nplt.scatter(xt, yt, label = 'data')\nplt.plot([0, 1], [theta_true[0], theta_true[0] + theta_true[1]], 'k--', label = 'true model')\nplt.plot([0, 1], [theta_numpy[0], theta_numpy[0] + theta_numpy[1]], 'r-', label = 'estimated model')\nplt.legend()\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "fnk7ZjTHbMxf", "tags": []}, "source": "Our regression model can be viewed as a neural network with only one linear layer. PyTorch already provides a [model of linear transformations](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear) that by default also includes a bias term. The following code creates such a linear transformation for one-dimensional inputs and outputs."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "j98bKgrPVodY", "tags": []}, "outputs": [], "source": "import torch.nn as nn\n\n# define an affine transformation\naffine = nn.Linear(1, 1)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "83YDtvgrWVJQ", "tags": []}, "source": "We can extract the internal parameters of the linear layer."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 69}, "colab_type": "code", "id": "tz01iKJgXcWg", "outputId": "9a0b15ba-5456-4e9c-ccf9-19375666d5fd", "tags": []}, "outputs": [], "source": "list(affine.parameters())"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "-C7abZucXxXp", "tags": []}, "source": "We see that by default gradients will be computed with respect to these parameters during backpropagation. Note that the parameters are randomly initialized and not set to 1.\n\nWe can make the correspondence to $\\beta_0$ and $\\beta_1$ a bit clearer."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 52}, "colab_type": "code", "id": "lyEs7SBvWYuX", "outputId": "6032ebe4-185c-4bc5-aabd-0d90bdef5024", "tags": []}, "outputs": [], "source": "theta0 = affine.bias.data\ntheta1 = affine.weight.data\nprint(f\"theta0: {theta0}\")\nprint(f\"theta1: {theta1}\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "FO6Gw_4JW2cd", "tags": []}, "source": "Conveniently, in contrast to our implementation above, PyTorch's linear layer does not require us to add a column of ones to our data matrix. Instead we can work with the given data directly, after converting it to a PyTorch tensor and reshaping it to a matrix with one column."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "colab_type": "code", "id": "khBO5mM-Y_I4", "outputId": "411db133-d655-4164-e965-7e36fbfe2bb2", "tags": []}, "outputs": [], "source": "# build data matrix\nX = torch.from_numpy(xt).float().view(-1, 1)\n\n# apply the affine transformation to the data\naffine(X)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "A4BKKgtdZX4P", "tags": []}, "source": "We can rewrite the training procedure using this linear layer."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 474}, "colab_type": "code", "id": "xzG4f7S3bMxg", "outputId": "94235761-070d-4065-e322-0fbf8cfccaf3", "tags": []}, "outputs": [], "source": "# define the initial model parameters\naffine = nn.Linear(1, 1)\n\n# define the optimizer\noptimizer = optim.SGD(affine.parameters(), lr=0.1)\n\n# track the training loss\ntraining_loss = []\n\n# in multiple iterations\nfor i in range(1000):\n    # evaluate the mean squared error for the current parameters\n    loss = (affine(X) - Y).pow(2).mean()\n        \n    # update the statistic and print loss\n    training_loss.append(loss.item())\n    if i == 0 or (i + 1) % 100 == 0:\n      print(f\"Epoch {i + 1:4d}: training loss {training_loss[-1]: 9.6f}\")\n    \n    # perform backpropagation\n    loss.backward()\n\n    # perform a gradient descent step\n    optimizer.step()\n\n    # reset the gradient information\n    optimizer.zero_grad()\n    \nplt.figure()\nplt.plot(np.arange(1, len(training_loss) + 1), training_loss, 'o', label='training loss')\nplt.yscale('log')\nplt.xlabel('number of iterations')\nplt.legend()\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "SbBvep-SAHAu", "tags": []}, "source": "Let us check again that we obtain a reasonably well-trained model."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 269}, "colab_type": "code", "id": "qB3ujikOAPxn", "outputId": "f018bf0d-910c-4f76-d648-a196b6f27da1", "tags": []}, "outputs": [], "source": "# extract theta0 and theta1 from the linear layer\ntheta0 = affine.bias.detach().numpy()\ntheta1 = affine.weight.detach().numpy()\n\nplt.scatter(xt, yt, label = 'data')\nplt.plot([0, 1], [theta_true[0], theta_true[0] + theta_true[1]], 'k--', label = 'true model')\nplt.plot([0, 1], [theta0, theta0 + theta1], 'r-', label = 'estimated model')\nplt.legend()\nplt.show()"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "0TlmA6QeMM0g", "tags": []}, "source": "Note that we have performed linear regression by iteratively minimizing the mean squared error cost function instead of using the closed form solution given by the normal equations. This numerical approach can be used also with other models and cost functions to find the parameters, also in problems where no closed form solution is available."}], "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "celltoolbar": "Tags", "colab": {"collapsed_sections": [], "name": "Introduction to PyTorch.ipynb", "provenance": []}, "kernelspec": {"display_name": "lab-sml", "language": "python", "name": "lab-sml"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.7"}}, "nbformat": 4, "nbformat_minor": 1}