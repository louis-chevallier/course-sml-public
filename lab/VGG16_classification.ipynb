{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "%matplotlib inline\nimport torch\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image\n\nimport json\nimport os\nfrom urllib.request import urlopen\nfrom urllib.error import URLError"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "mYz8s5ZwzIww", "tags": []}, "source": "# Real world image classification\n\nThis scipt classifies real world images using a pretrained neural network called VGG16. We start by downloading all its pretrained parameters, if they have not been downloaded yet, and then load the model."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "6LRwlk8Ay1mi", "tags": []}, "outputs": [], "source": "# check if we can access the pretrained VGG16 weights\ntry:\n    f = open('/it/student/sml/models/vgg16-397923af.pth', 'r')\nexcept OSError:\n    pass\nelse:\n    print(\"using the pretrained VGG16 weights in /it/student/sml/\")\n    f.close()\n    os.environ['TORCH_HOME'] = '/it/student/sml/'\n\nvgg16 = models.vgg16(pretrained=True)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "JUUAh7R7z8Ho", "tags": []}, "source": "We switch the network to evaluation mode (that disables dropout and other features specific for training) and print a summary of the network architecture."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": []}, "outputs": [], "source": "vgg16.eval()"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "The pretrained VGG16 model [expects](https://pytorch.org/docs/master/torchvision/models.html#classification)\n>  input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]` and `std = [0.229, 0.224, 0.225]`. \n\nThus we define the following set of preprocessing steps, that resizes an input image, crops it to the requested size, converts the pixels to values in [0, 1], and then normalizes them accordingly. "}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "preprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "To be able to analyze the predictions of the model, we load [curated list of human-readable labels](https://github.com/anishathalye/imagenet-simple-labels) for the 1000 classes of the ImageNet dataset with which the model has been trained."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": true, "tags": []}, "outputs": [], "source": "with urlopen('https://git.io/JvBFb') as f:\n    LABELS = json.load(f)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "qaBtBVkizxrF", "tags": []}, "source": "We load an image that we want to classify, either from a local folder or the internet."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "# URL of the image that we want to analyze\n# prepend 'file:' to local paths (e.g., use 'file:./gorilla.jpeg')\nimage_url = 'https://w.wiki/HaK'\n\n# some examples from Wikipedia:\n# Gorilla https://w.wiki/HaK\n# Uppsala cathedral https://w.wiki/HaV\n# African bush elephant https://w.wiki/HaH\n# Pelle Svansl\u00f6s https://w.wiki/HaG\n# Hedgehog https://w.wiki/HaJ\n# Suspension bridge https://w.wiki/HaQ\n\ntry:\n    with Image.open(urlopen(image_url)) as im:\n        # The following fixes some problems when loading images:\n        # https://stackoverflow.com/a/64598016\n        image = im.convert(\"RGB\")\nexcept (URLError, OSError):\n    print(\"please provide a valid URL or local path\")\nelse:    \n    print(f\"{image.mode} image of size {image.size}\")\n    plt.imshow(np.asarray(image))\n    plt.xticks([])\n    plt.yticks([])\n    plt.show()"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "We preprocess the image, retrieve the predicted probabilities from the VGG16 model."}, {"cell_type": "code", "execution_count": null, "metadata": {"scrolled": false, "tags": []}, "outputs": [], "source": "# perform the pre-processing and form a \"batch\" of one single image\nX = preprocess(image).unsqueeze(0)\n\n# obtain the predicted probabilities for the image\nwith torch.no_grad():\n    G = F.softmax(vgg16(X), dim=1)[0]"}, {"cell_type": "markdown", "metadata": {"tags": []}, "source": "We plot the image and print the human-readable labels of the top5 predictions."}, {"cell_type": "code", "execution_count": null, "metadata": {"tags": []}, "outputs": [], "source": "plt.imshow(image)\nplt.xticks([])\nplt.yticks([])\nplt.show()\n\nfor (p, y) in zip(*(G.topk(5))):\n    print(f\"{LABELS[y.item()]} ({100 * p.item():.2f}%)\")"}], "metadata": {"@webio": {"lastCommId": null, "lastKernelId": null}, "anaconda-cloud": {}, "celltoolbar": "Tags", "colab": {"name": "VGG16_classification.ipynb", "provenance": [], "version": "0.3.2"}, "kernelspec": {"display_name": "lab-sml", "language": "python", "name": "lab-sml"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.9.7"}}, "nbformat": 4, "nbformat_minor": 1}